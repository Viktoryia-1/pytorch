{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbc9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Переписать загрузку данных с python функций на Dataset и Dataloader и применить сеть с attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "46787857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bbad412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 64\n",
    "epochs = 30\n",
    "latent_dim = 256\n",
    "num_samples = 10000\n",
    "ex = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7fb13a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392306"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/rus-eng/rus.txt',mode='r', encoding='utf-8') as file:\n",
    "    lines = file.read().split('\\n')\n",
    "len(lines)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0419f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_vocab = set()\n",
    "output_vocab = set()\n",
    "\n",
    "for line in lines[:num_samples]:\n",
    "    input_txt, target_txt, __ = line.split('\\t')\n",
    "    target_txt = '\\t' + target_txt + '\\n'\n",
    "    input_texts.append(input_txt)\n",
    "    target_texts.append(target_txt)\n",
    "    for word in input_txt.split():\n",
    "#         word = str(word).lower()\n",
    "#         word = ''.join(i for i in lines if i not in ex)\n",
    "        input_vocab.add(word.strip())\n",
    "    for word in target_txt.split():\n",
    "#         word = str(word).lower()\n",
    "#         word = ''.join(i for i in lines if i not in ex)\n",
    "        output_vocab.add(word.strip())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9f89b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_index = {word: i + 2 for i, word in enumerate(input_vocab)}\n",
    "output_index = {word: i + 2 for i, word in enumerate(output_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "230bce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnRus(Dataset):\n",
    "    def __init__(self, input_text, input_idx, target_text, target_idx):\n",
    "        self.input_text = input_text\n",
    "        self.input_idx = input_idx\n",
    "        self.target_text = target_text\n",
    "        self.target_idx = target_idx\n",
    "        \n",
    "     \n",
    "    @classmethod\n",
    "    def get_index(self, sentence, vocab, idx):\n",
    "        return [vocab.get(word, 0) for word in sentence[idx].split(' ')]\n",
    "    \n",
    "    @classmethod\n",
    "    def add_one(self, sentence, vocab, idx):\n",
    "        indexes = self.get_index(sentence, vocab, idx)\n",
    "        indexes.append(1)\n",
    "        return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
    "    \n",
    "    def final_tensor(self, idx):\n",
    "        input_tensor = self.add_one(self.input_text, self.input_idx, idx)\n",
    "        output_tensor = self.add_one(self.target_text, self.target_idx, idx)\n",
    "        return (input_tensor, output_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_text)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.final_tensor(idx)\n",
    "        return pair\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3049b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = EnRus(input_text=input_texts, input_idx=input_index, target_text=target_texts, target_idx=output_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "62b14ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = DataLoader(data, batch_size=30, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8efa2b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_).view(1, 1, -1)\n",
    "        output, hidden = self.lstm(embedded, hidden.view(1, 1, 1, -1))\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "176568fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=10):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        #output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b731355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=10):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[0]])\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == 1:\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cec5980f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000 10000%) 2.8416\n",
      "(2000 20000%) 4.5871\n",
      "(3000 30000%) 2.5645\n",
      "(4000 40000%) 3.0038\n",
      "(5000 50000%) 1.6322\n",
      "(6000 60000%) 1.5926\n",
      "(7000 70000%) 2.8010\n",
      "(8000 80000%) 4.1900\n",
      "(9000 90000%) 2.6429\n",
      "(10000 100000%) 0.2757\n",
      "(11000 110000%) 3.1192\n",
      "(12000 120000%) 3.3673\n",
      "(13000 130000%) 1.4466\n",
      "(14000 140000%) 3.3976\n",
      "(15000 150000%) 3.2623\n",
      "(16000 160000%) 0.3130\n",
      "(17000 170000%) 2.1639\n",
      "(18000 180000%) 2.3759\n",
      "(19000 190000%) 2.4264\n",
      "(20000 200000%) 1.6752\n",
      "(21000 210000%) 1.5989\n",
      "(22000 220000%) 5.1185\n",
      "(23000 230000%) 2.0581\n",
      "(24000 240000%) 1.2077\n",
      "(25000 250000%) 4.0137\n",
      "(26000 260000%) 2.0311\n",
      "(27000 270000%) 3.2472\n",
      "(28000 280000%) 2.1888\n",
      "(29000 290000%) 5.1681\n",
      "(30000 300000%) 3.0166\n",
      "(31000 310000%) 3.0915\n",
      "(32000 320000%) 1.3094\n",
      "(33000 330000%) 3.5077\n",
      "(34000 340000%) 3.9526\n",
      "(35000 350000%) 3.1285\n",
      "(36000 360000%) 0.1574\n",
      "(37000 370000%) 2.0406\n",
      "(38000 380000%) 1.7910\n",
      "(39000 390000%) 0.3538\n",
      "(40000 400000%) 0.7997\n",
      "(41000 410000%) 2.8695\n",
      "(42000 420000%) 4.0073\n",
      "(43000 430000%) 3.0234\n",
      "(44000 440000%) 3.2112\n",
      "(45000 450000%) 0.1918\n",
      "(46000 460000%) 0.1160\n",
      "(47000 470000%) 1.8276\n",
      "(48000 480000%) 2.2300\n",
      "(49000 490000%) 3.0632\n",
      "(50000 500000%) 1.8020\n",
      "(51000 510000%) 3.3987\n",
      "(52000 520000%) 2.1903\n",
      "(53000 530000%) 3.3184\n",
      "(54000 540000%) 0.1420\n",
      "(55000 550000%) 1.6387\n",
      "(56000 560000%) 1.7375\n",
      "(57000 570000%) 1.6789\n",
      "(58000 580000%) 4.5667\n",
      "(59000 590000%) 0.1416\n",
      "(60000 600000%) 1.8712\n",
      "(61000 610000%) 2.3680\n",
      "(62000 620000%) 3.4632\n",
      "(63000 630000%) 2.1733\n",
      "(64000 640000%) 3.4652\n",
      "(65000 650000%) 4.6943\n",
      "(66000 660000%) 1.2073\n",
      "(67000 670000%) 1.1626\n",
      "(68000 680000%) 2.2725\n",
      "(69000 690000%) 2.9290\n",
      "(70000 700000%) 2.3155\n",
      "(71000 710000%) 0.1567\n",
      "(72000 720000%) 0.9987\n",
      "(73000 730000%) 0.1530\n",
      "(74000 740000%) 2.7247\n",
      "(75000 750000%) 4.1028\n",
      "(76000 760000%) 4.1142\n",
      "(77000 770000%) 0.1343\n",
      "(78000 780000%) 4.8205\n",
      "(79000 790000%) 1.3324\n",
      "(80000 800000%) 3.2146\n",
      "(81000 810000%) 1.7402\n",
      "(82000 820000%) 4.5910\n",
      "(83000 830000%) 2.5366\n",
      "(84000 840000%) 1.5932\n",
      "(85000 850000%) 3.7096\n",
      "(86000 860000%) 2.7264\n",
      "(87000 870000%) 1.2967\n",
      "(88000 880000%) 3.5039\n",
      "(89000 890000%) 3.1524\n",
      "(90000 900000%) 1.3481\n",
      "(91000 910000%) 1.8093\n",
      "(92000 920000%) 2.7762\n",
      "(93000 930000%) 5.6414\n",
      "(94000 940000%) 2.3107\n",
      "(95000 950000%) 3.2559\n",
      "(96000 960000%) 3.1217\n",
      "(97000 970000%) 0.1818\n",
      "(98000 980000%) 0.6347\n",
      "(99000 990000%) 2.3648\n"
     ]
    }
   ],
   "source": [
    "# Здесь обработка только через Dataset, ниже - попытка в DataLoader\n",
    "# Качество особо не улучшилось\n",
    "encoder = EncoderRNN(len(input_index)+2, 30)\n",
    "attn_decoder1 = AttnDecoderRNN(30, len(output_index)+2, dropout_p=0.1)\n",
    "\n",
    "#attn_decoder1 = DecoderRNN(len(output_vocab2index)+2, 30)\n",
    "\n",
    "encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=0.01)\n",
    "decoder_optimizer = torch.optim.SGD(attn_decoder1.parameters(), lr=0.01)\n",
    "training_pairs = np.random.randint(0, len(input_texts), size=100000)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "print_loss_total = 0\n",
    "for i in range(100000):    \n",
    "        input_tensor, target_tensor = data[training_pairs[i]]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "               attn_decoder1, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "    \n",
    "        print_loss_avg = print_loss_total / 1\n",
    "        print_loss_total = 0\n",
    "        if i != 0 and i % 1000 == 0:\n",
    "            print('(%d %d%%) %.4f' % (i, i / 10 * 100, print_loss_avg))\n",
    "            #print(f'Epoch {i} Loss {print_loss_avg}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b02e6d29",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [4, 1] at entry 0 and [3, 1] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lw/sfx9kxt17jj8cb5s59ll44q80000gp/T/ipykernel_62546/4169627846.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/gb/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/gb/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/gb/env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/gb/env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/gb/env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/gb/env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [4, 1] at entry 0 and [3, 1] at entry 1"
     ]
    }
   ],
   "source": [
    "# Моя любимая ошибка размерности, которую я не одолела\n",
    "# Хочу увидеть решение!!\n",
    "encoder = EncoderRNN(len(input_index)+2, 30)\n",
    "attn_decoder1 = AttnDecoderRNN(30, len(output_index)+2, dropout_p=0.1)\n",
    "\n",
    "#attn_decoder1 = DecoderRNN(len(output_vocab2index)+2, 30)\n",
    "\n",
    "encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=0.01)\n",
    "decoder_optimizer = torch.optim.SGD(attn_decoder1.parameters(), lr=0.01)\n",
    "training_pairs = np.random.randint(0, len(input_texts), size=10000)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "print_loss_total = 0\n",
    "for i in range(10000): \n",
    "    for index, (x, y) in enumerate(final_data):\n",
    "        input_tensor, target_tensor = x, y\n",
    "  \n",
    "        loss = train(input_tensor,\n",
    "                     target_tensor, \n",
    "                     encoder,\n",
    "               attn_decoder1, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "    \n",
    "        print_loss_avg = print_loss_total / 1\n",
    "        print_loss_total = 0\n",
    "        if i != 0 and i % 1000 == 0:\n",
    "            print('(%d %d%%) %.4f' % (i, i / 10 * 100, print_loss_avg))\n",
    "            #print(f'Epoch {i} Loss {print_loss_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd9e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fc7003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
